## 一、背景：我们为什么需要一个「文档记忆入口」

现状（你现在的系统）大概是这样几层：

* **视觉感知层**：`o` 模型做人脸 embedding + 聚类识别，物体/场景标签 → 挂在时间帧或片段上；
* **听觉层**：ASR 语音转文本；
* **文本记忆层**：对话、系统日志等走传统文本 → embedding → Graph / MemoryEntry；
* **上层记忆基础设施**：STKG / TKG、审计、TTL、Reaper、多租户等。

现在缺的那一块，是一个**“文档 / 屏幕 / PDF / 网页截图”专用的入口**：

> 用户上传一个 PDF / 网页 / 合同 / 手写笔记 / 截图，
> → 系统自动解析 + 压缩
> → 写入记忆图，后续可以被检索和引用。

DeepSeek-OCR 正好提供了一个强力组件：

* 它把**长文本 / 文档渲染成图片**，再用视觉编码器 DeepEncoder 压成很少的 vision tokens，
  再用 3B MoE 解码回文本/结构化结果。([arXiv][1])
* 实验表明：文本 token 数 ≤ 视觉 token 数 10 倍时，OCR 精度可达 ~97%；
  即使压到 20 倍，精度仍有 ~60%。([arXiv][1])
* 这本质上是一种**“用视觉 token 承载长文本上下文”的新范式**，非常适合做**文档的第一步处理和冷存压缩**。([arXiv][2])

所以，我们要做的不是“推翻现有系统”，而是：

> **在现有多模态记忆 OS 上，加一条「文档记忆入口 + 光学压缩层」。**

---

## 二、DeepSeek-OCR 给我们的三点关键启示

### 1. 文档 = “可以被高比率压缩的记忆块”

传统 RAG 的思路是：

> 文档 → 分段 → 文本 embedding → 索引 / 召回 → 塞回 LLM 上下文

DeepSeek-OCR 的思路是：

> 文档 / 历史对话 → 渲染成图片 → DeepEncoder → 少量 vision tokens
> → 需要时再解压成文本 / Markdown / 结构化输出。([arXiv][1])

对我们意味着：

* **文档在存储层，本质上可以“先压成视觉 latent”**，而不是全部展开成文本 token；
* 对于很长的合同、报告、网页 session，
  我们可以只保留少数 vision tokens，或者保存压缩图像本身 + 部分结构化摘要。

### 2. 上下文成本 = “压缩比可调的工程参数”

论文给出了清晰的**压缩比 ↔ OCR 精度曲线**：
10× 内 ~97%，20× 约 60%。([arXiv][1])

这对我们的“记忆经济学 / 遗忘曲线”是个非常直接的增强：

* 过去：我们理论上说“信息损失 L 要控制在某个阈值”，但缺少可操作的压缩曲线；
* 现在：可以把“压缩比”当成一个可调的 knob：

  * 近期 / 高价值文档：用 6–10× 压缩（近乎无损）；
  * 很久以前 / 低价值：压到 15–20×，接受一定信息损失，换取成本优势。

### 3. 它是一个 **OCR-oriented VLM，不是通用视觉编码器**

* DeepSeek-OCR 的设计重心是：**文字密集（文档、屏幕、图表、公式、化学式等）**。([arXiv][2])
* 训练数据约 70% 是 OCR，20% general vision，10% text-only：
  有一定的图像描述 / 检测 / grounding 能力，但**强烈偏向文本与结构化解析**。([技术栈][3])

这意味着：

* 适合：**PDF / 扫描件 / UI / 网页 / 论文 / 表格 / PPT 截图 / 白板 / 手写笔记**；
* 不适合替掉：我们现在的**人脸识别、场景解析、动作理解**等通用视觉感知。

---

## 三、和现有系统怎么「拼起来」？——顶层架构视角

从架构上，我们可以把“文档记忆入口”看成 STKG 下新增的一个“**光学文档层（Optical Document Layer）**”。

### 1. 现有大图：多模态 STKG

你现在的记忆图更像这样：

* 节点：Person / FaceCluster / Object / Scene / Room / Event / TimeSlice / Utterance / Knowledge…
* 边：who–where–when–what，跨模态证据链；
* 底层：Qdrant 做向量检索，Neo4j 做图结构；上层有 TTL、Reaper、Audit、多租户。

视觉和听觉部分已经比较成熟：

* 视觉：`o` 模型做人脸 embedding + 聚类 + 物体/场景标签；
* 听觉：ASR 产出 Utterance，挂到时间轴和事件上。

### 2. 新增一条“文档入口”管线

我们要加的是**第三条感知入口**，专门处理上传文档（含 PDF/图片/网页截图）：

**(1) 文档接入层**

* 用户上传：

  * PDF / Office 文档；
  * 图片：扫描件、手机拍的文档/白板/屏幕；
  * 网页：HTML 或截图。
* 预处理：

  * PDF/HTML → 页面渲染为图像（每页一图或多图）；
  * 图片 → 归一化裁剪、分辨率调节。

**(2) DeepSeek-OCR 解析 + 压缩**

* 根据业务选择模式：

  * Tiny/Small：64/100 tokens，适合多数普通文档；
  * Base/Large：256/400 tokens，适合精细图表/公式。([技术栈][3])
* 输出两类东西：

  1. **结构化文本结果**：纯文本 + Markdown + 表格/公式/图表解析；([arXiv][1])
  2. **光学压缩表示**：

     * 视觉 latent（vision tokens）；
     * 或压缩后的文档图像（1024×1024 / 640×640 等），可视为“光学快照”。

**(3) 写入 STKG / Memory 系统**

* 文本侧：

  * 把结构化 OCR 结果拆成 Document / Section / Paragraph / Table / Figure 等节点；
  * 建立与 Person / Project / Event / TimeSlice 的关系（来源、引用、作者、发生时间…）；
  * 为每个节点生成文本 embedding，写入 Qdrant，走现有的文本记忆流程。
* 光学侧：

  * 为每一页 / 关键段落建立 `MediaSegment(image)` 节点；
  * 将对应的 vision tokens / 压缩图像作为属性挂在节点上（指向外部存储或 blob）；
  * 将其标记为“冷存 / 归档层”，用于低成本回溯和长上下文压缩。

**(4) 检索与召回**

* 快速检索（高频路径）：

  * 仍然使用文本 embedding + BM25 + 图扩展；
  * 也可以使用“文档 → 事件 → 对话”的路径做 STKG 检索。
* 深度回溯（低频但高价值）：

  * 当需要完整重构某份文档 / 某段历史上下文时：

    * 首先通过图和文本索引定位对应的 Document / Section / MediaSegment；
    * 再调用 DeepSeek-OCR / VLM 用保留的 vision tokens / 压缩图像解出更细致内容。

---

## 四、回答我们之前几个关键疑问（整理版）

### 问题 1：DeepSeek-OCR 会不会“推翻”我们现有记忆架构？

**不会。**

* 它解决的是“**长文本 / 文档上下文如何以更低 token 成本存储和解码**”的问题；([arXiv][1])
* 我们解决的是“**多用户、多模态、多年的记忆如何结构化、可检索、可治理**”的问题：

  * 多租户隔离；
  * 审计 / 回滚 / TTL / Reaper；
  * STKG 上的时空实体与事件；
  * Graph 检索与推理。

它更像是我们记忆系统里的一个**超强 I/O + 压缩模块**，而不是替代 STKG 或 Memory infra 的“新 OS”。

---

### 问题 2：可不可以“直接把所有视频帧丢给 DeepSeek-OCR，当通用视觉 encoder 用”？

**不建议。**

* DeepSeek-OCR 是 **OCR-oriented VLM**：
  绝大部分 capacity 押在“从图里把文字/结构化内容解压出来 + 高压缩比”这件事上。([技术栈][4])
* 它确实保留了一些 general vision 能力（描述、检测、grounding），因为有 20% general vision 数据；([技术栈][3])
  但论文**没有系统评估**在不同压缩比下的通用视觉语义保留度，更没有做人脸识别/场景理解 benchmark。([arXiv][2])

对你来说：

* 人脸聚类/识别、跨帧 tracking、细粒度场景理解：
  继续使用 `o` 模型 + 专门管线更稳；
* DeepSeek-OCR 更适合插在：

  * 文档 / 屏幕帧 / UI / 白板；
  * 以及长文本上下文的压缩归档。

---

### 问题 3：用视觉 latent 代替文本 embedding 做 RAG / 记忆存储靠谱吗？

**短答案：可以作为“冷存层 + 压缩层”，但不要完全替掉文本 embedding。**

* 论文证明了：在 10× 压缩时几乎无损地恢复文本，这说明**vision tokens 能承载丰富文本信息**；([arXiv][1])
* 但：

  * 论文没做“vision tokens 做语义检索 vs 文本 embedding”的系统对比；
  * 也没做“不同 token 数下，通用视觉语义损失曲线”的 benchmark。([arXiv][2])

较稳妥的策略是：

* 热层：继续用文本 embedding + 索引（Qdrant）；
* 冷层：额外保存 vision tokens / 压缩图像，作为大体“文档快照”；
* 需要深挖时，用图 + 文本先定位，再调用 DeepSeek-OCR / VLM 对快照做精细解析。

---

## 五、面向“文档记忆入口”的具体设计建议（MVP 级）

下面这一节可以直接作为后续设计文档的骨架。

### 1. MVP 功能目标

* 支持用户上传：PDF / 图片（扫描、截图） / 网页（HTML + 截图）；
* 自动：

  1. 解析文档结构；
  2. 写入记忆系统（STKG + MemoryEntry）；
  3. 通过现有检索接口（/search、Graph 查询）快速召回；
* 作为“全模态记忆”的一个入口，和视觉/听觉/对话记忆统一在同一套 STKG 下。

### 2. Pipeline（建议 v1 版本）

**Step 0：文档接入 & 预分类**

* 根据 MIME / 扩展名区分：PDF / 图片 / HTML / 其他；
* PDF/HTML → 渲染每页图像；
  图片 → 标准化为规定分辨率（例如 1024×1024）。

**Step 1：DeepSeek-OCR 解析**

* 对每一页 / 关键截图调用 DeepSeek-OCR（Tiny/Small 模式即可）：

  * 得到：文本、Markdown、表格、图表、公式等；
  * 同时保留原始图像或压缩图像（可选保存 vision tokens，视工程难度而定）。([arXiv][1])

**Step 2：结构化建模 → 写入 STKG**

* 建 `Document` 节点：类型（合同/报告/邮件/论文）、作者、创建时间、来源；
* 按章节/页/段落建 `Section` / `Paragraph` / `Table` / `Figure` 节点；
* 建 `MediaSegment` 节点存放页面/截图（+ 光学压缩属性）；
* 将文档与：

  * 用户（作者/接收者）；
  * 运行任务 / 项目 / 会话；
  * 时间/地点（如果能从内容或元数据推断）
    通过边连接起来。

**Step 3：文本 embedding + 索引**

* 对 Paragraph / Section 层生成文本 embedding → 写入 Qdrant；
* 按 Document 级别维护倒排索引（BM25）；
* 通过现有 Hybrid Search + Graph 扩展把文档记忆纳入统一检索框架。

**Step 4：检索与召回**

* 普通问答 / RAG：

  * user query → rewrite → 文本检索 → 命中文档段落 → 作为 evidence 提供给 VLM；
* 时空类问题（例如“上周那份合同的修改内容”）：

  * 通过 STKG 找到相关事件 → 事件关联的 Document / Section → 再做局部检索。

---

## 六、战略性方向前瞻（中长期）

最后给几条更“前瞻一点”的方向，方便你后续规划 roadmap：

### 方向 1：把光学压缩纳入“记忆生命周期”的正式阶段

在你已有的遗忘机制（Lazy Decay / Soft TTL / Reinforcement / Reaper）之外，加一个第五个动作：

> **Optical Downsampling（光学降采样）**

* 高价值 / 近期：保留完整文本 + 结构化节点；
* 中价值：保留摘要 + 低压缩比 optical 快照；
* 低价值 / 很久之前：只保留高压缩比快照，或者最终 Reaper 删除。

可以直接用 DeepSeek-OCR 提供的“压缩比 ↔ 精度”曲线作为工程指南。([arXiv][1])

### 方向 2：统一「屏幕 / 文档 / 历史对话」的处理范式

长远看，可以把：

* 用户屏幕录制 / 产品 UI；
* 上传的 PDF/网页；
* 长对话历史（聊天记录、会议记录）；

统一成一种“**光学上下文**”形式：渲染为 2D 图像 → DeepSeek-OCR 压缩 → 作为长期记忆的底层载体。([arXiv][2])

STKG 在上面做结构化抽象：人物、事件、场景、任务状态等。

### 方向 3：构建“文档 × 多模态”的联合检索体验

当文档入口成熟之后，可以提供一些更高级的能力，例如：

* “帮我找到**那天我们讨论某个需求时投过的那份 PPT**”：
  对话 → 时间 → 会议事件 → 关联的屏幕录制截帧 → 对应文档。
* “把过去一个月里，用户在屏幕上看到但没有 explicit 提问、但频繁盯着看的内容，总结成一份阅读报告”：
  屏幕 OCR + 视线/停留时间统计 + 文档聚合。

这些都是“**文档记忆 × 视觉/听觉记忆**”的交叉点，会是你们系统区别于纯文本 RAG 的杀手锏。

---

## 七、总结

> **我们不把 DeepSeek-OCR 当成“新的记忆系统”，而是把它当成“文档与长上下文的光学压缩入口”，
> 在现有多模态记忆 OS（STKG + Memory infra）的基础上，加出一条高效的「文档记忆入口」，
> 既能深度解析用户上传文档，又能在长期上用视觉 token 大幅降低上下文与存储成本。**



[1]: https://arxiv.org/abs/2510.18234?utm_source=chatgpt.com "DeepSeek-OCR: Contexts Optical Compression"
[2]: https://arxiv.org/html/2510.18234v1?utm_source=chatgpt.com "DeepSeek-OCR: Contexts Optical Compression - arXiv.org"
[3]: https://jishuzhan.net/article/1984202140127133697?utm_source=chatgpt.com "DeepSeek-OCR 论文精读与实践：用“光学上下文压缩”把长 ..."
[4]: https://jishuzhan.net/article/1983362482451316737?utm_source=chatgpt.com "综述：deepSeek-OCR，paddle-OCR，VLM - 技术栈"
