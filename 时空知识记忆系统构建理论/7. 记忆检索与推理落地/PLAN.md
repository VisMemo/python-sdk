# 记忆检索与推理落地计划（对标清单 → Demo 可回答）

> Status: Pre-production / 方案级
> 目标：让 demo_website 能回答《记忆检索与推理对标清单》的 22 个问题，并给出可解释证据链。

---

## 0. 范围与前提

**范围**：视频 `/ingest` → 生成 TKG → demo_website 可提问并得到可解释答案。  
**前提**：Memory Service 与 Graph API 可用，/ingest 成功写入图。  
**非目标**：不是生产级性能与长期存储方案；先把“能答对问题”跑通。

---

## 1. 现状能力快照（基于当前代码）

**已有能力**
- 图结构：`Segment`、`Event`、`Entity(Person/Object)`、`Evidence(face/voice/object/scene)`、`Utterance`。
- 关系：`CONTAINS_EVIDENCE`、`BELONGS_TO_ENTITY`、`SPOKEN_BY`、`INVOLVES`（基础）。
- API：`/graph/v0/*` 基础查询 + `/graph/v0/explain/event`、`/graph/v0/explain/first_meeting`。
- ASR：有时间戳 + speaker_track_id；支持声纹聚类。
- VLM：可生成事件/片段语义描述（当前未强绑定 ASR 输入）。

**主要缺口（影响 L1-L5）**
- Place/场景结构薄弱（OpenCLIP 缺依赖时为空）。
- 事件类型/动作标签不稳定（无法直接回答行为问题）。
- 没有状态节点/时序关系链（L2/L5 无法可靠推断）。
- 多跳关系（共现、第一次相遇）未构建或未系统化。
- 跨模态对齐（对话→人物→事件）仍偏弱。
- 否定逻辑与“缺失推理”无数据基准。

---

## 2. L1-L5 理想 vs 当前差距

| Level | 当前现状 | 关键缺口 | 理想目标 |
|---|---|---|---|
| L1 基础事实 | 有事件/片段/语音 | 地点/对象属性/事件类型不足 | 时间+地点+对象+语音可直接检索 |
| L2 时序/状态 | 有时间戳 | 缺状态/动作/Next 关系 | 可回答“前后顺序/时长/最后状态” |
| L3 多跳关系 | explain 端点有雏形 | 共现/首次相遇/认识度不足 | 多跳关系可解释且稳定 |
| L4 语义泛化 | 有 VLM 描述 | 情绪/OCR/环境语义缺失 | 语义标签与多模态对齐 |
| L5 否定/边缘 | 无“缺失基线” | 无时间覆盖、无对话否定判定 | 能回答“是否发生/是否缺失” |

---

## 3. 问题 → 图结构 → 查询路径（可执行方案）

> 约定：查询优先用 `/graph/v0/*`；需要文本检索时用 `/search`/`/speech_search`。

### L1 基础事实检索
1) “上周五我去了哪些地方？”  
- 需要：`Event` + `Place` + 时间范围（timeslice 或事件时间）  
- 查询：`/graph/v0/events?start=...&end=...` → 事件关联 Place  
- 缺口：Place 生成不稳定 → 需补 Scene/Place 抽取。

2) “我在视频里提到‘人工智能’是在什么时候？”  
- 需要：`Utterance` 文本检索 + 时间戳  
- 查询：`/speech_search`（或 `/search`）→ Utterance → 关联 Event/Segment  
- 缺口：Utterance 未与 Event 解释链强绑定。

3) “画面里出现过红色的杯子吗？”  
- 需要：Object Evidence + 属性（color）  
- 查询：`/object_search` 或 Evidence 过滤 → Segment  
- 缺口：颜色/属性未稳定抽取。

4) “昨天下午跟我开会的人是谁？”  
- 需要：Event{type=meeting}+INVOLVES  
- 查询：`/graph/v0/events?type=meeting&start=...&end=...`  
- 缺口：事件类型缺失（需 VLM/规则分类）。

### L2 时序与状态流转
5) “我回家后做的第一件事是什么？”  
- 需要：事件序列 + “回家”事件 + NEXT_EVENT  
- 查询：定位“回家”事件 → NEXT_EVENT  
- 缺口：NEXT_EVENT / 事件排序未统一构建。

6) “我昨天玩手机玩了多久？”  
- 需要：Event{action=use_phone}+duration  
- 查询：事件过滤 + SUM(duration)  
- 缺口：动作分类/时长聚合不完善。

7) “我的车钥匙现在在哪？”  
- 需要：Object/State 节点 + 最后状态  
- 查询：查询“钥匙”相关事件按时间倒序  
- 缺口：State 节点未建模。

8) “出门前我锁门了吗？”  
- 需要：锁门/出门事件序列  
- 查询：在“出门”前窗口内找 Lock/Unlock  
- 缺口：动作事件识别能力不足。

### L3 多跳推理与间接关系
9) “我和 Alice 是怎么认识的？”  
- 需要：共现事件 + first meeting  
- 查询：`/graph/v0/explain/first_meeting?me_id=...&other_id=...`  
- 缺口：共现统计/人物身份标注不足。

10) “有没有我不认识的人在我的客厅里？”  
- 需要：Place=客厅 + 人物 known/unknown  
- 查询：事件→Place 过滤 → 人物属性筛选  
- 缺口：人物“认识度”标签缺失。

11) “经常和 Bob 一起出现的戴眼镜的男人？”  
- 需要：共现统计 + 人物属性(戴眼镜)  
- 查询：co-occurs topK → 属性过滤  
- 缺口：共现边未构建，属性识别缺失。

12) “这个快递箱是谁拿进来的？”  
- 需要：Object=box + 施动者关系  
- 查询：围绕 box 的事件 → 找到 Actor  
- 缺口：动作事件与施动者关系不足。

### L4 语义泛化与多模态对齐
13) “我看起来很焦虑的时候都在做什么？”  
- 需要：情绪标签 + 事件语义  
- 查询：情绪过滤 → 事件/动作统计  
- 缺口：情绪识别未引入。

14) “上周我不小心摔倒或弄坏东西的片段？”  
- 需要：意外类语义标签  
- 查询：语义类目过滤（fall/drop/break）  
- 缺口：语义类别缺失。

15) “找写着 Wifi 密码的便利贴。”  
- 需要：OCR + 位置追踪  
- 查询：OCR 文本匹配 → 物体/场景定位  
- 缺口：OCR 未集成。

16) “我说‘好冷’的时候，真的冷吗？”  
- 需要：语音文本 + 环境/穿着状态  
- 查询：Utterance → 时间窗口内环境证据对比  
- 缺口：环境状态未建模。

### L5 否定与边缘情况
17) “上个月我有哪天完全没有出门？”  
- 需要：天粒度时间覆盖 + 出门事件  
- 查询：日期全集 - 出门事件日期  
- 缺口：时间覆盖基准未建立。

18) “聚会上我有没有和李四说话？”  
- 需要：对话关系或 Utterance→Entity 双方  
- 查询：Party 事件范围内找 TALK_TO/Utterance  
- 缺口：对话关系边未构建。

19) “有没有谁动过我桌子上的文件？”  
- 需要：文件对象 + 触碰/移动事件  
- 查询：围绕文件事件，筛选 Actor != Me  
- 缺口：小动作识别不足。

20) “只有我和女朋友两个人的场景？”  
- 需要：事件参与者集合计数  
- 查询：事件聚合参与者 = 2 且为特定人  
- 缺口：参与者集合稳定性与实体身份标注。

21) “最近一周作息变化？”  
- 需要：按天/周的行为统计  
- 查询：事件聚合统计 → 对比两周  
- 缺口：作息/行为标签体系缺失。

22) “只看工作账号下的会议和文档？”  
- 需要：tenant/domain/scope 过滤  
- 查询：Graph API + tenant/domain filter  
- 缺口：domain/scope 标注与 UI 过滤未统一。

---

## 4. 分阶段落地计划（系统化路径）

### Phase 0：稳定可写（P0）
- 目标：/ingest → 图可查。  
- 必要条件：`MEMA_MEMORY_API_URL` 配置正确；GraphUpsert 可用。

### Phase 1：L1-L2 可答（P1）
- **结构补齐**：
  - Scene/Place 抽取稳定化（补 OpenCLIP 依赖或替代方案）。
  - 事件类型/动作标签（仅 VLM 输出 `Event.type`/`tags`）。
  - 建立 `NEXT_EVENT` / `timeslices`（用 `/graph/v0/admin/build_*`）。
- **查询路径**：
  - 标准化“问题 → Graph API 查询模板”。
  - Demo UI：问题卡片 → 结果 + 证据链。

### Phase 2：L3-L4 可答（P2）
- **关系增强**：
  - 共现边 `CO_OCCURS` / 首次相遇解释链。
  - 人物属性（known/unknown、glasses 等）。
- **多模态对齐**：
  - VLM 输入强制包含 ASR+speaker diarization（生成“谁对谁说了什么”的事件描述）。
  - OCR 接入（文本对象）。
  - 情绪识别接入（基础情绪标签即可）。

### Phase 3：L5 可答（P3）
- **否定逻辑基线**：
  - 时间覆盖基准（按天生成“空事件”或覆盖索引）。
  - 对话关系边（TALK_TO）。
- **聚合能力**：
  - 参与者集合统计与过滤。
  - 行为模式对比（周对周）。

---

## 5. Demo 端体验要求（必须满足）

**Definition of Done（核心）**
- 事件节点有 VLM 描述，描述包含 ASR 语音文本作为输入依据。
- 用户在事件详情中能清楚看到“发生了什么”。
- 用户点击人物 → 能看到该人物的语音片段（含时间戳）与对应的画面/face 证据。

**解释链展示**
- 每个回答必须包含：事件/片段 → 证据（图/音/文本） → 关系链（可折叠）。

---

## 6. 验证方式（每一层都有可跑的“答案用例”）

- L1/L2：直接 Graph API 查询 + 单条问题回放。
- L3/L4：至少 3 个多跳问题的 end-to-end 结果 + 解释链。
- L5：至少 2 个“否定/缺失”问题跑通（有明确失败解释）。

---

## 7. 立即行动清单（第一版）

1) 补齐 Place/Scene 抽取（解决 OpenCLIP 缺依赖问题）。  
2) 用 VLM 生成 `Event.tags/type`，并明确规则写入图。  
3) 在写入后触发 `build_timeslices` + `build_event_relations` + `build_cooccurs`。  
4) Demo UI 增加“问题 → 查询模板”路由与解释链视图。  
5) 强制 VLM 输入包含 ASR + speaker diarization（用于“谁说了什么”）。

---

## 8. 风险与对策

- **识别质量不稳定**：先用规则+阈值收敛，再引入更复杂模型。  
- **否定问题无基线**：先做“时间覆盖索引”，再谈否定推理。  
- **解释链不可信**：只展示真实存在的节点/边，禁止生成式“脑补”。

---

## 9. 版本记录

- v0.1：初版计划，对标清单 → 可执行检索路径（2025-12-25）
