# æ•°æ®åº“äº‘åŒ–æ”¹é€ 

## ğŸ¯ äº‘åŒ–ç­–ç•¥

### è¯„ä¼°ç»“æœ
**å½“å‰æ•°æ®åº“**ï¼š
- Neo4j (å›¾æ•°æ®åº“) - è®°å¿†ç³»ç»Ÿ
- Qdrant (å‘é‡æ•°æ®åº“) - ç›¸ä¼¼åº¦æœç´¢
- In-Memory (ä¸´æ—¶æ•°æ®) - ç¼“å­˜

**äº‘åŒ–ç›®æ ‡**ï¼š
- é«˜å¯ç”¨ (99.9%+ SLA)
- è‡ªåŠ¨å¤‡ä»½ä¸æ¢å¤
- å¼¹æ€§ä¼¸ç¼©
- é™ä½è¿ç»´æˆæœ¬

## ğŸ—ƒï¸ Neo4jäº‘åŒ–

### AWS Neptune vs æ‰˜ç®¡Neo4j vs è‡ªå»º
| ç‰¹æ€§ | AWS Neptune | æ‰˜ç®¡Neo4j | è‡ªå»ºAurora/è‡ªç®¡ |
|------|-------------|-----------|------------------|
| **æˆæœ¬** | ä¸­ | é«˜ | ä¸­-é«˜ |
| **æ€§èƒ½** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| **å¤šç§Ÿæˆ·** | éœ€è‡ªå»º | åŸç”Ÿæ”¯æŒ | éœ€è‡ªå»º |
| **è¿ç§»éš¾åº¦** | ä¸­ | ä½ | é«˜ |

**æ¨èæ–¹æ¡ˆï¼šè‡ªå»ºNeo4jé›†ç¾¤**
- æˆæœ¬å¯æ§
- å®Œå…¨æ§åˆ¶
- æ”¯æŒå¤šç§Ÿæˆ·

### Neo4jé›†ç¾¤æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è´Ÿè½½å‡è¡¡å™¨ (NLB)            â”‚
â”‚           (TCP 7687)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Core 1 â”‚        â”‚ Core 2 â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                 â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Read 1  â”‚        â”‚ Read 2  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¶æ„è¯´æ˜:
- 3ä¸ªCoreèŠ‚ç‚¹ (ä»²è£)
- 2ä¸ªåªè¯»å‰¯æœ¬ (è¯»æ‰©å±•)
- è·¨å¯ç”¨åŒºéƒ¨ç½²
- è‡ªåŠ¨æ•…éšœè½¬ç§»
```

### Terraformé…ç½®
```hcl
# neo4j-cluster.tf

# Neo4j CoreèŠ‚ç‚¹ (us-east-1a)
resource "aws_instance" "neo4j_core_1" {
  ami           = data.aws_ami.neo4j.id
  instance_type = "r5.2xlarge"
  key_name      = aws_key_pair.deploy.key_name
  subnet_id     = aws_subnet.private_1a.id

  root_block_device {
    volume_type = "gp3"
    volume_size = 500
    encrypted   = true
  }

  user_data = <<-EOF
    #!/bin/bash
    sudo tee /etc/neo4j/neo4j.conf > /dev/null <<EOL
    dbms.mode=CORE
    dbms.cluster.discovery.proposed_initial_discovery_members=10.0.1.10:5000,10.0.2.10:5000,10.0.3.10:5000
    dbms.cluster.minimum_core_cluster_size_at_formation=3
    dbms.cluster.minimum_core_cluster_size_at_runtime=3
    dbms.connector.bolt.listen_address=0.0.0.0:7687
    dbms.connector.http.listen_address=0.0.0.0:7474
    dbms.connector.https.listen_address=0.0.0.0:7493
    dbms.memory.heap.initial_size=8g
    dbms.memory.heap.max_size=8g
    dbms.memory.pagecache.size=12g
    EOL
    sudo systemctl restart neo4j
  EOF

  tags = {
    Name = "neo4j-core-1"
    Role = "neo4j-core"
  }
}

# Neo4jåªè¯»å‰¯æœ¬
resource "aws_instance" "neo4j_read_1" {
  count         = 2
  ami           = data.aws_ami.neo4j.id
  instance_type = "r5.xlarge"
  key_name      = aws_key_pair.deploy.key_name
  subnet_id     = aws_subnet.private_1b.id

  user_data = <<-EOF
    #!/bin/bash
    sudo tee /etc/neo4j/neo4j.conf > /dev/null <<EOL
    dbms.mode=READ_REPLICA
    dbms.cluster.discovery.proposed_initial_discovery_members=10.0.1.10:5000,10.0.2.10:5000,10.0.3.10:5000
    dbms.connector.bolt.listen_address=0.0.0.0:7687
    dbms.connector.http.listen_address=0.0.0.0:7474
    dbms.memory.heap.initial_size=4g
    dbms.memory.heap.max_size=4g
    dbms.memory.pagecache.size=6g
    EOL
    sudo systemctl restart neo4j
  EOF

  tags = {
    Name = "neo4j-read-${count.index + 1}"
    Role = "neo4j-replica"
  }
}
```

### å¤šç§Ÿæˆ·å®ç°
```python
# æ•°æ®åº“è¿æ¥ç®¡ç†
class Neo4jTenantManager:
    def __init__(self, cluster_endpoints: List[str]):
        self.endpoints = cluster_endpoints
        self.driver_pool: Dict[str, "GraphDatabase.driver"] = {}

    async def get_tenant_driver(self, tenant_id: str):
        """è·å–ç§Ÿæˆ·ä¸“ç”¨è¿æ¥"""
        if tenant_id not in self.driver_pool:
            driver = GraphDatabase.driver(
                self.endpoints[0],  # ä½¿ç”¨ä¸»èŠ‚ç‚¹
                auth=("neo4j", os.getenv("NEO4J_PASSWORD")),
                max_connection_pool_size=50
            )
            self.driver_pool[tenant_id] = driver

        return self.driver_pool[tenant_id]

    async def execute_query(self, tenant_id: str, query: str, params: dict):
        """æ‰§è¡Œç§Ÿæˆ·æŸ¥è¯¢ï¼ˆè‡ªåŠ¨æ·»åŠ è¿‡æ»¤ï¼‰"""
        driver = await self.get_tenant_driver(tenant_id)

        # ç¡®ä¿æŸ¥è¯¢åŒ…å«ç§Ÿæˆ·è¿‡æ»¤
        if "tenant_id" not in query and "MATCH" in query.upper():
            # ç®€å•æ£€æŸ¥ï¼Œæ›´å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨æŸ¥è¯¢è§£æå™¨
            query = query.replace(
                "MATCH (",
                f"MATCH (n {{tenant_id: '{tenant_id}'"
            )

        async with driver.session() as session:
            result = await session.run(query, **params)
            return await result.data()

# ä½¿ç”¨ç¤ºä¾‹
class MemoryService:
    def __init__(self, db_manager: Neo4jTenantManager):
        self.db = db_manager

    async def search_memory(self, query: str, tenant_id: str):
        cypher = """
            MATCH (m:Memory)
            WHERE m.content CONTAINS $query
            RETURN m
            ORDER BY m.created_at DESC
            LIMIT 10
        """
        return await self.db.execute_query(tenant_id, cypher, {"query": query})
```

## ğŸ” Qdrantäº‘åŒ–

### äº‘æ‰˜ç®¡æœåŠ¡
**é€‰æ‹©Qdrant Cloud**
- âœ… å®˜æ–¹æ‰˜ç®¡æœåŠ¡
- âœ… è‡ªåŠ¨ä¼¸ç¼©
- âœ… å†…ç½®ç›‘æ§
- âŒ æˆæœ¬è¾ƒé«˜

### è‡ªå»ºQdranté›†ç¾¤
```yaml
# docker-compose.qdrant.yml
version: '3.8'

services:
  qdrant-1:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data_1:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__CLUSTER__ENABLED=true
      - QDRANT__CLUSTER__NODE_ID=0
      - QDRANT__CLUSTER__INITIAL_NODES=qdrant-1:6335,qdrant-2:6335,qdrant-3:6335
    networks:
      - qdrant_net

  qdrant-2:
    image: qdrant/qdrant:latest
    ports:
      - "6335:6333"
      - "6336:6334"
    volumes:
      - qdrant_data_2:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__CLUSTER__ENABLED=true
      - QDRANT__CLUSTER__NODE_ID=1
      - QDRANT__CLUSTER__INITIAL_NODES=qdrant-1:6335,qdrant-2:6335,qdrant-3:6335
    networks:
      - qdrant_net

  qdrant-3:
    image: qdrant/qdrant:latest
    ports:
      - "6337:6333"
      - "6338:6334"
    volumes:
      - qdrant_data_3:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__CLUSTER__ENABLED=true
      - QDRANT__CLUSTER__NODE_ID=2
      - QDRANT__CLUSTER__INITIAL_NODES=qdrant-1:6335,qdrant-2:6335,qdrant-3:6335
    networks:
      - qdrant_net

volumes:
  qdrant_data_1:
  qdrant_data_2:
  qdrant_data_3:

networks:
  qdrant_net:
    driver: bridge
```

### å¤šç§Ÿæˆ·Collectionè®¾è®¡
```python
class QdrantMultiTenant:
    def __init__(self, host: str, port: int = 6333):
        self.client = QdrantClient(host=host, port=port)

    def create_tenant_collection(self, tenant_id: str, collection_name: str):
        """ä¸ºç§Ÿæˆ·åˆ›å»ºä¸“å±Collection"""
        full_name = f"{tenant_id}_{collection_name}"

        self.client.create_collection(
            collection_name=full_name,
            vectors_config=VectorParams(
                size=768,  # å‘é‡ç»´åº¦
                distance=Distance.COSINE
            ),
            optimizers_config=OptimizersConfig(
                default_segment_number=2,
                max_segment_size=100000,  # 10ä¸‡å‘é‡
            ),
            replication_factor=2,  # 2ä»½å‰¯æœ¬
            consistency=1  # å†™ä¸€è‡´æ€§
        )

        return full_name

    def store_vector(self, tenant_id: str, collection_name: str,
                    vector_id: str, vector: List[float],
                    payload: dict):
        """å­˜å‚¨å‘é‡ï¼ˆè‡ªåŠ¨æ·»åŠ ç§Ÿæˆ·IDï¼‰"""
        full_name = f"{tenant_id}_{collection_name}"

        payload_with_tenant = {
            **payload,
            "tenant_id": tenant_id
        }

        self.client.upsert(
            collection_name=full_name,
            points=[PointStruct(
                id=vector_id,
                vector=vector,
                payload=payload_with_tenant
            )]
        )

    def search_vector(self, tenant_id: str, collection_name: str,
                     query_vector: List[float], limit: int = 10):
        """æœç´¢å‘é‡ï¼ˆä»…é™å½“å‰ç§Ÿæˆ·ï¼‰"""
        full_name = f"{tenant_id}_{collection_name}"

        return self.client.search(
            collection_name=full_name,
            query_vector=query_vector,
            limit=limit,
            query_filter=Filter(
                must=[FieldCondition(
                    key="tenant_id",
                    match=MatchValue(value=tenant_id)
                )]
            )
        )
```

## ğŸ’¾ æ•°æ®è¿ç§»

### è¿ç§»ç­–ç•¥
```python
# æ•°æ®è¿ç§»å·¥å…·
class DatabaseMigration:
    def __init__(self, source_neo4j, target_neo4j, source_qdrant, target_qdrant):
        self.source_neo4j = source_neo4j
        self.target_neo4j = target_neo4j
        self.source_qdrant = source_qdrant
        self.target_qdrant = target_qdrant

    async def migrate_tenant(self, tenant_id: str):
        """è¿ç§»å•ä¸ªç§Ÿæˆ·æ•°æ®"""
        print(f"å¼€å§‹è¿ç§»ç§Ÿæˆ·: {tenant_id}")

        # 1. è¿ç§»Neo4jæ•°æ®
        print("è¿ç§»Neo4jæ•°æ®...")
        await self.migrate_neo4j_tenant(tenant_id)

        # 2. è¿ç§»Qdrantæ•°æ®
        print("è¿ç§»Qdrantæ•°æ®...")
        await self.migrate_qdrant_tenant(tenant_id)

        # 3. éªŒè¯è¿ç§»
        print("éªŒè¯è¿ç§»...")
        await self.verify_migration(tenant_id)

        print(f"ç§Ÿæˆ· {tenant_id} è¿ç§»å®Œæˆ")

    async def migrate_neo4j_tenant(self, tenant_id: str):
        """ä»æºNeo4jè¯»å–å¹¶å†™å…¥ç›®æ ‡"""
        async with self.source_neo4j.session() as session:
            # è¯»å–æ‰€æœ‰è®°å¿†
            result = await session.run(
                "MATCH (m:Memory {tenant_id: $tenant_id}) RETURN m",
                tenant_id=tenant_id
            )
            memories = await result.data()

        # å†™å…¥ç›®æ ‡
        async with self.target_neo4j.session() as session:
            for memory in memories:
                await session.run(
                    "CREATE (m:Memory $props)",
                    props=memory["m"]
                )

    async def migrate_qdrant_tenant(self, tenant_id: str):
        """è¿ç§»Qdrant Collection"""
        # åˆ—å‡ºç§Ÿæˆ·çš„æ‰€æœ‰Collection
        source_collections = await self.source_qdrant.get_collections()
        tenant_collections = [c.name for c in source_collections.collections
                            if c.name.startswith(tenant_id)]

        for collection_name in tenant_collections:
            # åˆ›å»ºæ–°Collection
            self.target_qdrant.create_tenant_collection(tenant_id, collection_name)

            # è¿ç§»å‘é‡æ•°æ®
            points = self.source_qdrant.scroll(collection_name)[0]
            if points:
                self.target_qdrant.client.upsert(
                    collection_name=f"{tenant_id}_{collection_name}",
                    points=points
                )
```

### é›¶åœæœºè¿ç§»
```python
class ZeroDowntimeMigration:
    def __init__(self):
        self.migration_status = {}

    async def start_migration(self):
        """å¯åŠ¨è¿ç§»"""
        # 1. åŒæ­¥å¤åˆ¶ï¼ˆæ–°æ—§åŒæ—¶å†™ï¼‰
        await self.enable_replication()

        # 2. æ•°æ®è¿ç§»
        tenants = await self.get_all_tenants()
        for tenant_id in tenants:
            await self.migrate_tenant_async(tenant_id)

        # 3. ç­‰å¾…åŒæ­¥å®Œæˆ
        await self.wait_for_sync()

        # 4. åˆ‡æ¢è¯»å†™
        await self.switch_to_cloud()

    async def enable_replication(self):
        """å¯ç”¨åŒå†™æ¨¡å¼"""
        # å†™å…¥ä¸­é—´ä»¶ï¼ŒåŒæ—¶å†™å…¥æ–°æ—§æ•°æ®åº“
        self.write_proxy = DualWriteProxy(
            source_db=self.local_db,
            target_db=self.cloud_db
        )

    async def wait_for_sync(self):
        """ç­‰å¾…æ•°æ®åŒæ­¥"""
        while True:
            lag = await self.check_data_lag()
            if lag < 100:  # å·®è·å°äº100æ¡è®°å½•
                break
            await asyncio.sleep(10)
```

## ğŸ”„ å¤‡ä»½ä¸æ¢å¤

### è‡ªåŠ¨å¤‡ä»½
```bash
#!/bin/bash
# backup-neo4j.sh

# Neo4jå¤‡ä»½
neo4j-admin database backup \
    --database=neo4j \
    --backup-dir=/backups/$(date +%Y-%m-%d) \
    --check-consistency=true

# ä¸Šä¼ åˆ°S3
aws s3 sync /backups/ s3://moyan-backups/neo4j/ \
    --storage-class STANDARD_IA

# æ¸…ç†æœ¬åœ°å¤‡ä»½ï¼ˆä¿ç•™7å¤©ï¼‰
find /backups -type d -mtime +7 -exec rm -rf {} \;
```

### æ¢å¤æµç¨‹
```python
class DisasterRecovery:
    def __init__(self):
        self.s3_client = boto3.client('s3')

    async def restore_tenant(self, tenant_id: str, timestamp: str):
        """æ¢å¤ç§Ÿæˆ·æ•°æ®"""
        backup_date = timestamp.split('T')[0]

        # 1. ä»S3ä¸‹è½½å¤‡ä»½
        backup_path = await self.download_backup(tenant_id, backup_date)

        # 2. åœæ­¢åº”ç”¨
        await self.stop_application()

        # 3. æ¢å¤æ•°æ®åº“
        await self.restore_neo4j(backup_path, tenant_id)
        await self.restore_qdrant(backup_path, tenant_id)

        # 4. å¯åŠ¨åº”ç”¨
        await self.start_application()

        # 5. éªŒè¯æ¢å¤
        await self.verify_restore(tenant_id)

    async def verify_restore(self, tenant_id: str):
        """éªŒè¯æ¢å¤ç»“æœ"""
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        memory_count = await self.count_memories(tenant_id)
        if memory_count == 0:
            raise Exception("æ¢å¤å¤±è´¥ï¼šæ²¡æœ‰æ•°æ®")

        # éªŒè¯è¿æ¥
        test_result = await self.test_api_call(tenant_id)
        if not test_result:
            raise Exception("æ¢å¤å¤±è´¥ï¼šAPIä¸å¯ç”¨")
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### è¯»å†™åˆ†ç¦»
```python
class DatabaseRouter:
    def __init__(self):
        self.read_replicas = [
            "neo4j-read-1:7687",
            "neo4j-read-2:7687"
        ]
        self.write_master = "neo4j-core-1:7687"
        self.read_index = 0

    def get_read_endpoint(self):
        """è½®è¯¢è·å–è¯»å‰¯æœ¬"""
        endpoint = self.read_replicas[self.read_index]
        self.read_index = (self.read_index + 1) % len(self.read_replicas)
        return endpoint

    async def execute_query(self, query_type: str, tenant_id: str, **kwargs):
        """æ ¹æ®æŸ¥è¯¢ç±»å‹è·¯ç”±"""
        if query_type == "write":
            # å†™å…¥æ“ä½œèµ°ä¸»åº“
            return await self.execute_on_endpoint(
                self.write_master, query, **kwargs
            )
        else:
            # è¯»å–æ“ä½œèµ°å‰¯æœ¬
            endpoint = self.get_read_endpoint()
            return await self.execute_on_endpoint(
                endpoint, query, **kwargs
            )
```

### ç¼“å­˜ç­–ç•¥
```python
# Redisç¼“å­˜é…ç½®
class CacheManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_ttl = 3600  # 1å°æ—¶

    async def cache_memory_search(self, tenant_id: str, query: str, result):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        cache_key = f"search:{tenant_id}:{hash(query)}"
        await self.redis.setex(
            cache_key,
            self.default_ttl,
            json.dumps(result)
        )

    async def get_cached_result(self, tenant_id: str, query: str):
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = f"search:{tenant_id}:{hash(query)}"
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        return None

    async def invalidate_cache(self, tenant_id: str):
        """å¤±æ•ˆç§Ÿæˆ·ç¼“å­˜"""
        pattern = f"search:{tenant_id}:*"
        await self.redis.delete(*await self.redis.keys(pattern))
```

## âœ… å®æ–½æ¸…å•
- [ ] éƒ¨ç½²Neo4jé›†ç¾¤
- [ ] éƒ¨ç½²Qdranté›†ç¾¤
- [ ] é…ç½®å¤šç§Ÿæˆ·éš”ç¦»
- [ ] å®æ–½æ•°æ®è¿ç§»å·¥å…·
- [ ] æµ‹è¯•é›¶åœæœºè¿ç§»
- [ ] é…ç½®è‡ªåŠ¨å¤‡ä»½
- [ ] å®æ–½ç¾éš¾æ¢å¤æµç¨‹
- [ ] æ€§èƒ½æµ‹è¯•ä¸è°ƒä¼˜
- [ ] ç›‘æ§å‘Šè­¦é…ç½®
- [ ] è¿ç»´æ–‡æ¡£ç¼–å†™
- [ ] æˆæœ¬ä¼˜åŒ–
- [ ] åº”æ€¥é¢„æ¡ˆæ¼”ç»ƒ
