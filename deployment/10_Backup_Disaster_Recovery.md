# å¤‡ä»½ä¸ç¾éš¾æ¢å¤

## ğŸ›¡ï¸ å¤‡ä»½ç­–ç•¥

### 3-2-1å¤‡ä»½åŸåˆ™
```
3ä»½æ•°æ®å‰¯æœ¬
â”œâ”€â”€ åŸå§‹æ•°æ®
â”œâ”€â”€ æœ¬åœ°å¤‡ä»½ (å¯ç”¨åŒºA)
â””â”€â”€ å¼‚åœ°å¤‡ä»½ (å¯ç”¨åŒºB) + å†·å¤‡ä»½ (S3 Glacier)

2ç§ä¸åŒä»‹è´¨
â”œâ”€â”€ å—å­˜å‚¨ (EBS)
â””â”€â”€ å¯¹è±¡å­˜å‚¨ (S3)

1ä»½å¼‚åœ°å¤‡ä»½
â”œâ”€â”€ è·¨åŒºåŸŸ (us-east-1 â†’ us-west-2)
â””â”€â”€ ç”Ÿå‘½å‘¨æœŸç®¡ç†
```

## ğŸ’¾ è‡ªåŠ¨åŒ–å¤‡ä»½

### Neo4jå¤‡ä»½
```bash
#!/bin/bash
# backup-neo4j.sh

set -e

BACKUP_DIR="/backups/neo4j/$(date +%Y-%m-%d)"
S3_BUCKET="moyan-backups"
REGION="us-east-1"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½æ‰€æœ‰ç§Ÿæˆ·æ•°æ®åº“
TENANTS=$(aws dynamodb scan --table-name moyan-tenants --query 'Items[*].tenant_id.S' --output text)

for TENANT_ID in $TENANTS; do
    echo "å¤‡ä»½ç§Ÿæˆ·: $TENANT_ID"

    # Neo4jæ•°æ®åº“å¤‡ä»½
    neo4j-admin database backup \
        --database=tenant_${TENANT_ID}_graph \
        --backup-dir=$BACKUP_DIR \
        --check-consistency=true \
        --pagecache=4g

    # åˆ›å»ºå¿«ç…§
    tar -czf ${BACKUP_DIR}/tenant_${TENANT_ID}.tar.gz \
        ${BACKUP_DIR}/tenant_${TENANT_ID}_graph

    # ä¸Šä¼ åˆ°S3
    aws s3 cp ${BACKUP_DIR}/tenant_${TENANT_ID}.tar.gz \
        s3://${S3_BUCKET}/neo4j/$(date +%Y-%m-%d)/ \
        --storage-class STANDARD_IA

    # æ¸…ç†æœ¬åœ°æ–‡ä»¶
    rm -f ${BACKUP_DIR}/tenant_${TENANT_ID}.tar.gz
done

# æ¸…ç†7å¤©å‰çš„æœ¬åœ°å¤‡ä»½
find /backups -type d -mtime +7 -exec rm -rf {} +

echo "å¤‡ä»½å®Œæˆ"
```

### Qdrantå¤‡ä»½
```python
# backup_qdrant.py
import asyncio
from qdrant_client import QdrantClient
import boto3
from datetime import datetime

class QdrantBackup:
    def __init__(self, qdrant_url: str, s3_bucket: str):
        self.client = QdrantClient(url=qdrant_url)
        self.s3 = boto3.client('s3')
        self.bucket = s3_bucket

    async def backup_all_collections(self):
        """å¤‡ä»½æ‰€æœ‰Collection"""
        collections = self.client.get_collections()
        date_str = datetime.now().strftime('%Y-%m-%d')

        for collection in collections.collections:
            print(f"å¤‡ä»½Collection: {collection.name}")

            # åˆ›å»ºå¿«ç…§
            snapshot_info = self.client.create_snapshot(collection.name)
            snapshot_path = snapshot_info[0].location

            # ä¸‹è½½å¿«ç…§
            local_path = f"/tmp/{collection.name}.snapshot"
            await self.download_snapshot(snapshot_path, local_path)

            # ä¸Šä¼ åˆ°S3
            s3_key = f"qdrant/{date_str}/{collection.name}.snapshot"
            self.s3.upload_file(
                local_path,
                self.bucket,
                s3_key,
                ExtraArgs={'StorageClass': 'STANDARD_IA'}
            )

            # æ¸…ç†æœ¬åœ°æ–‡ä»¶
            os.remove(local_path)

        print("æ‰€æœ‰Collectionå¤‡ä»½å®Œæˆ")

    async def backup_specific_tenant(self, tenant_id: str):
        """å¤‡ä»½ç‰¹å®šç§Ÿæˆ·çš„æ•°æ®"""
        collections = self.client.get_collections()
        tenant_collections = [c for c in collections.collections
                            if c.name.startswith(tenant_id)]

        for collection in tenant_collections:
            # åªå¤‡ä»½è¯¥ç§Ÿæˆ·çš„Collections
            await self.backup_collection(collection.name)
```

### S3å¯¹è±¡å­˜å‚¨å¤‡ä»½
```python
# backup_s3.py
import boto3
from botocore.exceptions import ClientError

class S3Backup:
    def __init__(self, source_bucket: str, dest_bucket: str):
        self.source = boto3.client('s3')
        self.dest = boto3.client('s3')
        self.source_bucket = source_bucket
        self.dest_bucket = dest_bucket

    def sync_bucket(self, prefix: str = ""):
        """åŒæ­¥S3å­˜å‚¨æ¡¶"""
        paginator = self.source.get_paginator('list_objects_v2')

        for page in paginator.paginate(Bucket=self.source_bucket, Prefix=prefix):
            if 'Contents' in page:
                for obj in page['Contents']:
                    source_key = obj['Key']
                    dest_key = f"s3://{self.dest_bucket}/{source_key}"

                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    try:
                        self.dest.head_object(Bucket=self.dest_bucket, Key=source_key)
                        print(f"è·³è¿‡ (å·²å­˜åœ¨): {source_key}")
                        continue
                    except ClientError:
                        pass

                    # å¤åˆ¶å¯¹è±¡
                    print(f"å¤åˆ¶: {source_key}")
                    self.dest.copy({
                        'Bucket': self.source_bucket,
                        'Key': source_key
                    }, self.dest_bucket, source_key)

    def apply_lifecycle_policy(self):
        """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç­–ç•¥"""
        policy = {
            "Rules": [
                {
                    "ID": "backup-lifecycle",
                    "Status": "Enabled",
                    "Filter": {"Prefix": ""},
                    "Transitions": [
                        {
                            "Days": 30,
                            "StorageClass": "STANDARD_IA"
                        },
                        {
                            "Days": 90,
                            "StorageClass": "GLACIER"
                        },
                        {
                            "Days": 365,
                            "StorageClass": "DEEP_ARCHIVE"
                        }
                    ],
                    "Expiration": {
                        "Days": 2555  # 7å¹´
                    }
                }
            ]
        }

        self.dest.put_bucket_lifecycle_configuration(
            Bucket=self.dest_bucket,
            LifecycleConfiguration=policy
        )
```

## ğŸ”„ ç¾éš¾æ¢å¤ (DR)

### RTO/RPOç›®æ ‡
```yaml
RTO (æ¢å¤æ—¶é—´ç›®æ ‡):
  Tier 1 (æ ¸å¿ƒAPI): 15åˆ†é’Ÿ
  Tier 2 (å¢å€¼æœåŠ¡): 1å°æ—¶
  Tier 3 (åˆ†ææœåŠ¡): 4å°æ—¶

RPO (æ•°æ®ä¸¢å¤±ç›®æ ‡):
  Tier 1: < 5åˆ†é’Ÿ
  Tier 2: < 15åˆ†é’Ÿ
  Tier 3: < 1å°æ—¶

å¯ç”¨æ€§ç›®æ ‡:
  SLA: 99.9%
  å¹´åœæœºæ—¶é—´: 8.76å°æ—¶
  æœˆåœæœºæ—¶é—´: 43.2åˆ†é’Ÿ
  å‘¨åœæœºæ—¶é—´: 10.1åˆ†é’Ÿ
```

### æ•…éšœè½¬ç§»æµç¨‹
```python
# disaster_recovery.py
import asyncio
import boto3
from datetime import datetime

class DisasterRecovery:
    def __init__(self):
        self.route53 = boto3.client('route53')
        self.ecs = boto3.client('ecs')
        self.rds = boto3.client('rds')
        self.primary_region = 'us-east-1'
        self.secondary_region = 'us-west-2'

    async def detect_failure(self):
        """æ£€æµ‹ä¸»åŒºåŸŸæ•…éšœ"""
        checks = [
            self.check_api_health(),
            self.check_database_health(),
            self.check_storage_health()
        ]

        results = await asyncio.gather(*checks, return_exceptions=True)

        if all(results):
            return False  # å¥åº·

        # è‡³å°‘ä¸€ä¸ªæ£€æŸ¥å¤±è´¥
        print("æ£€æµ‹åˆ°æ•…éšœï¼Œå¼€å§‹ç¾éš¾æ¢å¤æµç¨‹")
        return True

    async def execute_failover(self):
        """æ‰§è¡Œæ•…éšœè½¬ç§»"""
        print("å¼€å§‹æ•…éšœè½¬ç§»åˆ°å¤‡ç”¨åŒºåŸŸ...")

        # 1. æ¿€æ´»å¤‡ç”¨æ•°æ®åº“
        await self.activate_standby_db()

        # 2. å¯åŠ¨å¤‡ç”¨åŒºåŸŸæœåŠ¡
        await self.start_secondary_services()

        # 3. åˆ‡æ¢DNS
        await self.update_dns_records()

        # 4. éªŒè¯æœåŠ¡
        await self.verify_failover()

        print("æ•…éšœè½¬ç§»å®Œæˆ")

    async def activate_standby_db(self):
        """æ¿€æ´»å¤‡ç”¨æ•°æ®åº“"""
        print("æ¿€æ´»å¤‡ç”¨æ•°æ®åº“...")

        # åˆ›å»ºæ•°æ®åº“å¿«ç…§
        response = self.rds.create_db_snapshot(
            DBInstanceIdentifier='moyan-db-primary',
            DBSnapshotIdentifier=f'pre-failover-{datetime.now().isoformat()}'
        )

        # æå‡å¤‡ç”¨å®ä¾‹ä¸ºä¸»å®ä¾‹
        self.rds.promote_read_replica(
            DBInstanceIdentifier='moyan-db-standby'
        )

        # ç­‰å¾…æ•°æ®åº“å°±ç»ª
        waiter = self.rds.get_waiter('db_instance_available')
        waiter.wait(DBInstanceIdentifier='moyan-db-standby')

    async def update_dns_records(self):
        """æ›´æ–°DNSè®°å½•æŒ‡å‘å¤‡ç”¨åŒºåŸŸ"""
        print("æ›´æ–°DNSè®°å½•...")

        zone_id = 'Z123456789'
        new_endpoint = 'api-dr.moyan.ai'

        # æ›´æ–°Route53è®°å½•
        self.route53.change_resource_record_sets(
            HostedZoneId=zone_id,
            ChangeBatch={
                'Changes': [{
                    'Action': 'UPSERT',
                    'ResourceRecordSet': {
                        'Name': 'api.moyan.ai',
                        'Type': 'CNAME',
                        'TTL': 300,
                        'ResourceRecords': [{'Value': new_endpoint}]
                    }
                }]
            }
        )

    async def verify_failover(self):
        """éªŒè¯æ•…éšœè½¬ç§»"""
        print("éªŒè¯æ•…éšœè½¬ç§»...")

        # æµ‹è¯•APIç«¯ç‚¹
        async with aiohttp.ClientSession() as session:
            async with session.get('https://api.moyan.ai/api/health') as resp:
                if resp.status == 200:
                    print("âœ… APIæœåŠ¡æ­£å¸¸")
                else:
                    raise Exception("APIæœåŠ¡å¼‚å¸¸")

        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        try:
            from neo4j import GraphDatabase
            driver = GraphDatabase.driver(
                "bolt://moyan-db-standby.cluster-xxx.us-west-2.rds.amazonaws.com:7687",
                auth=("neo4j", "password")
            )
            driver.verify_connectivity()
            print("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
        except Exception as e:
            raise Exception(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
```

### è·¨åŒºåŸŸå¤åˆ¶
```hcl
# ä¸»åŒºåŸŸé…ç½®
resource "aws_dynamodb_global_table" "tenant_table" {
  region     = var.primary_region
  table_name = "moyan-tenants"

  replication_group {
    region_name = var.primary_region
  }

  replication_group {
    region_name = var.secondary_region
  }
}

# S3è·¨åŒºåŸŸå¤åˆ¶
resource "aws_s3_bucket" "primary_bucket" {
  bucket = "moyan-primary"
  region = var.primary_region
}

resource "aws_s3_bucket_versioning" "primary_versioning" {
  bucket = aws_s3_bucket.primary_bucket.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_replication_configuration" "primary_replication" {
  # éœ€è¦å¼€å¯ç‰ˆæœ¬æ§åˆ¶
  role   = aws_iam_role.replication.arn
  bucket = aws_s3_bucket.primary_bucket.id

  rule {
    id     = "ReplicationRule"
    status = "Enabled"

    destination {
      bucket        = aws_s3_bucket.secondary_bucket.arn
      storage_class = "STANDARD_IA"
    }
  }
}
```

## ğŸ“Š å¤‡ä»½ç›‘æ§

### å¤‡ä»½çŠ¶æ€æ£€æŸ¥
```python
# backup_monitor.py
import boto3
from datetime import datetime, timedelta

class BackupMonitor:
    def __init__(self):
        self.s3 = boto3.client('s3')
        self.dynamodb = boto3.client('dynamodb')

    def check_backup_status(self):
        """æ£€æŸ¥å¤‡ä»½çŠ¶æ€"""
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        backup_prefix = f"backup/{yesterday}/"

        # æ£€æŸ¥S3å¤‡ä»½
        response = self.s3.list_objects_v2(
            Bucket='moyan-backups',
            Prefix=backup_prefix
        )

        if 'Contents' not in response:
            return {
                'status': 'failed',
                'message': 'æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶'
            }

        # éªŒè¯å¤‡ä»½å®Œæ•´æ€§
        total_size = sum(obj['Size'] for obj in response['Contents'])
        file_count = len(response['Contents'])

        return {
            'status': 'success',
            'file_count': file_count,
            'total_size_gb': total_size / 1024 / 1024 / 1024,
            'timestamp': datetime.now().isoformat()
        }

    def check_recovery_point(self):
        """æ£€æŸ¥æ¢å¤ç‚¹"""
        # æ£€æŸ¥æœ€æ–°çš„RPO
        db_snapshots = self.dynamodb.list_db_snapshots(
            DBInstanceIdentifier='moyan-db',
            MaxRecords=1
        )

        if db_snapshots['DBSnapshots']:
            latest_snapshot = db_snapshots['DBSnapshots'][0]
            snapshot_time = latest_snapshot['SnapshotCreateTime']
            rpo_minutes = (datetime.now() - snapshot_time).total_seconds() / 60

            return {
                'rpo_minutes': rpo_minutes,
                'rpo_compliant': rpo_minutes < 5,
                'last_snapshot': snapshot_time.isoformat()
            }

        return {
            'rpo_compliant': False,
            'message': 'æ²¡æœ‰æ‰¾åˆ°å¿«ç…§'
        }

    def generate_backup_report(self):
        """ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š"""
        status = self.check_backup_status()
        rpo = self.check_recovery_point()

        report = {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'backup_status': status,
            'recovery_point': rpo,
            'recommendations': []
        }

        if not status['status'] == 'success':
            report['recommendations'].append(
                'å¤‡ä»½å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¤‡ä»½è„šæœ¬å’Œå­˜å‚¨ç©ºé—´'
            )

        if not rpo['rpo_compliant']:
            report['recommendations'].append(
                f"RPOè¶…é™ ({rpo['rpo_minutes']:.1f}åˆ†é’Ÿ > 5åˆ†é’Ÿ)ï¼Œ"
                'å»ºè®®å¢åŠ å¤‡ä»½é¢‘ç‡'
            )

        return report
```

## ğŸš¨ æ¼”ç»ƒè®¡åˆ’

### DRæ¼”ç»ƒæµç¨‹
```yaml
DRæ¼”ç»ƒé¢‘ç‡:
  å°è§„æ¨¡æ¼”ç»ƒ: æ¯æœˆ
  å…¨æµç¨‹æ¼”ç»ƒ: æ¯å­£åº¦
  å¹´åº¦ç¾éš¾æ¨¡æ‹Ÿ: æ¯å¹´

æ¼”ç»ƒæ­¥éª¤:
  1. å‡†å¤‡é˜¶æ®µ (1å‘¨å‰):
     - é€šçŸ¥æ‰€æœ‰ç›¸å…³æ–¹
     - å‡†å¤‡æ¼”ç»ƒç¯å¢ƒ
     - åˆ¶å®šè¯¦ç»†è®¡åˆ’

  2. æ¼”ç»ƒæ‰§è¡Œ (1å¤©):
     - æ¨¡æ‹Ÿä¸»åŒºåŸŸæ•…éšœ
     - æ‰§è¡Œæ•…éšœè½¬ç§»
     - éªŒè¯æœåŠ¡å¯ç”¨æ€§
     - è®°å½•é—®é¢˜å’Œè€—æ—¶

  3. å¤ç›˜æ€»ç»“ (1å‘¨å†…):
     - åˆ†ææ¼”ç»ƒç»“æœ
     - è¯†åˆ«æ”¹è¿›ç‚¹
     - æ›´æ–°DRè®¡åˆ’
```

### æ¼”ç»ƒè„šæœ¬
```bash
#!/bin/bash
# dr-drill.sh

echo "å¼€å§‹DRæ¼”ç»ƒ..."
echo "æ—¶é—´: $(date)"

# 1. æ¨¡æ‹Ÿæ•…éšœ
echo "æ­¥éª¤1: æ¨¡æ‹Ÿä¸»åŒºåŸŸæ•…éšœ"
aws ecs update-service --cluster moyan-prod --service api-service --desired-count 0 --region us-east-1

# 2. ç­‰å¾…æ£€æµ‹
echo "æ­¥éª¤2: ç­‰å¾…æ•…éšœæ£€æµ‹ (30ç§’)"
sleep 30

# 3. éªŒè¯æ•…éšœ
echo "æ­¥éª¤3: éªŒè¯æ•…éšœ"
curl -f https://api.moyan.ai/api/health || echo "âœ… ç¡®è®¤APIä¸å¯ç”¨"

# 4. æ‰§è¡Œæ•…éšœè½¬ç§»
echo "æ­¥éª¤4: æ‰§è¡Œæ•…éšœè½¬ç§»"
python3 /opt/moyan/disaster_recovery.py --action failover

# 5. éªŒè¯æ–°æœåŠ¡
echo "æ­¥éª¤5: éªŒè¯æ•…éšœè½¬ç§»ç»“æœ"
sleep 30
curl -f https://api-dr.moyan.ai/api/health || echo "âŒ æ•…éšœè½¬ç§»å¤±è´¥"

# 6. æ¢å¤ä¸»æœåŠ¡
echo "æ­¥éª¤6: æ¢å¤ä¸»æœåŠ¡"
aws ecs update-service --cluster moyan-prod --service api-service --desired-count 4 --region us-east-1

# 7. åˆ‡æ¢å›ä¸»åŒºåŸŸ
echo "æ­¥éª¤7: åˆ‡æ¢å›ä¸»åŒºåŸŸ"
python3 /opt/moyan/disaster_recovery.py --action failback

# 8. æœ€ç»ˆéªŒè¯
echo "æ­¥éª¤8: æœ€ç»ˆéªŒè¯"
curl -f https://api.moyan.ai/api/health && echo "âœ… æœåŠ¡å·²æ¢å¤"

echo "DRæ¼”ç»ƒå®Œæˆ"
```

## ğŸ“‹ åˆè§„è¦æ±‚

### å¤‡ä»½ä¿ç•™ç­–ç•¥
```yaml
ä¿ç•™æœŸé™:
  æ—¥å¸¸å¤‡ä»½: 30å¤©
  å‘¨å¤‡ä»½: 12å‘¨
  æœˆå¤‡ä»½: 12ä¸ªæœˆ
  å¹´å¤‡ä»½: 7å¹´
  åˆè§„å¤‡ä»½: 10å¹´ (é‡‘è/åŒ»ç–—)

å­˜å‚¨ä½ç½®:
  å†·æ•°æ®: S3 Glacier (ä½æˆæœ¬)
  çƒ­æ•°æ®: S3 Standard-IA (å¿«é€Ÿæ¢å¤)
  åŠ å¯†: AES-256ç«¯åˆ°ç«¯åŠ å¯†
  åœ°åŸŸ: è‡³å°‘è·¨2ä¸ªåœ°ç†åŒºåŸŸ
```

## âœ… å®æ–½æ¸…å•
- [ ] å®æ–½Neo4jè‡ªåŠ¨å¤‡ä»½
- [ ] å®æ–½Qdrantå¿«ç…§å¤‡ä»½
- [ ] é…ç½®S3è·¨åŒºåŸŸå¤åˆ¶
- [ ] å»ºç«‹æ•…éšœè½¬ç§»æµç¨‹
- [ ] é…ç½®è‡ªåŠ¨å¥åº·æ£€æŸ¥
- [ ] å®æ–½DNSè‡ªåŠ¨åˆ‡æ¢
- [ ] å»ºç«‹ç›‘æ§å‘Šè­¦
- [ ] åˆ¶å®šæ¼”ç»ƒè®¡åˆ’
- [ ] è¿›è¡ŒDRæ¼”ç»ƒ
- [ ] ä¼˜åŒ–æ¢å¤æµç¨‹
- [ ] æ–‡æ¡£åŒ–æµç¨‹
- [ ] åŸ¹è®­è¿ç»´å›¢é˜Ÿ
